{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LIAORONGTIAN/Cookbook/blob/master/AdalFlow_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ü§ó Generator\n",
        "\n",
        "# Quick Links\n",
        "\n",
        "Github repo: https://github.com/SylphAI-Inc/AdalFlow\n",
        "\n",
        "Full Tutorials: https://adalflow.sylph.ai/index.html#\n",
        "\n",
        "Discord: https://discord.gg/ezzszrRZvT\n",
        "\n",
        "‚≠ê <i>Star us on <a href=\"https://github.com/SylphAI-Inc/AdalFlow\">Github</a> </i> ‚≠ê\n",
        "\n",
        "## üìñ Outline\n",
        "\n",
        "This is the code for tutorial: https://adalflow.sylph.ai/tutorials/generator.html\n",
        "\n",
        "It covers:\n",
        "* how to use generator with template, id, cache, and output parsing.\n",
        "\n",
        "* how to switch to different model provider.\n",
        "* how to create a Generator purely from json or yaml config.\n",
        "* A simple chatbot."
      ],
      "metadata": {
        "id": "_6H9X05agm6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "!pip install -U adalflow[groq]\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "ksh6MsOZKPsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup api key"
      ],
      "metadata": {
        "id": "mY7ApbSTrQ6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from getpass import getpass\n",
        "\n",
        "groq_api_key = getpass(\"Please enter your GROQ API key: \")\n",
        "\n",
        "\n",
        "os.environ['GROQ_API_KEY'] = groq_api_key\n",
        "\n",
        "print(\"API keys have been set.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBvJVhXHrTPc",
        "outputId": "934173a1-09fe-48e7-dc33-479c8f0de730"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter your GROQ API key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "API keys have been set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generator comes with default template\n",
        "\n",
        "All we need is a ``model_client`` and a ``model_kwargs`` where the parameters are directly passed to the API provider as it is.\n",
        "\n",
        "In default, generator comes with default template. You can visualize it with ``print(generator)``."
      ],
      "metadata": {
        "id": "NTn3LrP7KQor"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: `print(component)` is a great way to visualize its structure."
      ],
      "metadata": {
        "id": "dq8U_9MkuLaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import adalflow as adal\n",
        "from adalflow.components.model_client import GroqAPIClient\n",
        "\n",
        "generator = adal.Generator(\n",
        "    model_client=GroqAPIClient(),\n",
        "    model_kwargs={\"model\": \"llama3-8b-8192\"},\n",
        ")\n",
        "\n",
        "# print the generator structure\n",
        "print(generator)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqTHs7NSKw4G",
        "outputId": "6f794391-530b-48e1-ec5a-dc342b1e73e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cache_path: /root/.adalflow/cache_GroqAPIClient_llama3-8b-8192.db\n",
            "Generator(\n",
            "  model_kwargs={'model': 'llama3-8b-8192'}, \n",
            "  (prompt): Prompt(\n",
            "    template: <START_OF_SYSTEM_PROMPT>\n",
            "    {# task desc #}\n",
            "    {% if task_desc_str %}\n",
            "    {{task_desc_str}}\n",
            "    {% else %}\n",
            "    You are a helpful assistant.\n",
            "    {% endif %}\n",
            "    {#input format#}\n",
            "    {% if input_format_str %}\n",
            "    <INPUT_FORMAT>\n",
            "    {{input_format_str}}\n",
            "    </INPUT_FORMAT>\n",
            "    {% endif %}\n",
            "    {# output format #}\n",
            "    {% if output_format_str %}\n",
            "    <OUTPUT_FORMAT>\n",
            "    {{output_format_str}}\n",
            "    </OUTPUT_FORMAT>\n",
            "    {% endif %}\n",
            "    {# tools #}\n",
            "    {% if tools_str %}\n",
            "    <TOOLS>\n",
            "    {{tools_str}}\n",
            "    </TOOLS>\n",
            "    {% endif %}\n",
            "    {# example #}\n",
            "    {% if examples_str %}\n",
            "    <EXAMPLES>\n",
            "    {{examples_str}}\n",
            "    </EXAMPLES>\n",
            "    {% endif %}\n",
            "    {# chat history #}\n",
            "    {% if chat_history_str %}\n",
            "    <CHAT_HISTORY>\n",
            "    {{chat_history_str}}\n",
            "    </CHAT_HISTORY>\n",
            "    {% endif %}\n",
            "    {#contex#}\n",
            "    {% if context_str %}\n",
            "    <CONTEXT>\n",
            "    {{context_str}}\n",
            "    </CONTEXT>\n",
            "    {% endif %}\n",
            "    <END_OF_SYSTEM_PROMPT>\n",
            "    <START_OF_USER_PROMPT>\n",
            "    {% if input_str %}\n",
            "    {{input_str}}\n",
            "    {% endif %}\n",
            "    <END_OF_USER_PROMPT>\n",
            "    {# steps #}\n",
            "    {% if steps_str %}\n",
            "    <START_OF_ASSISTANT_STEPS>\n",
            "    {{steps_str}}\n",
            "    <END_OF_ASSISTANT_STEPS>\n",
            "    {% endif %}\n",
            "    , prompt_variables: ['examples_str', 'output_format_str', 'input_format_str', 'context_str', 'chat_history_str', 'tools_str', 'task_desc_str', 'steps_str', 'input_str']\n",
            "  )\n",
            "  (model_client): GroqAPIClient()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The default prompt is built on [Jinja2](https://jinja.palletsprojects.com/en/3.1.x/). We've included why using Jinja2 [here](https://lightrag.sylph.ai/tutorials/prompt.html#why-jinja2).\n",
        "\n",
        "\n",
        "Now let's check the the final prompt with prompt variables:"
      ],
      "metadata": {
        "id": "Xs2aN3ko_lUY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgroQyoW5HrD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "7dfea481-f190-4ad5-c95d-e808e3c31d09"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<START_OF_SYSTEM_PROMPT>\\nYou are a helpful assistant.\\n<END_OF_SYSTEM_PROMPT>\\n<START_OF_USER_PROMPT>\\nWhat is LLM? Explain in one sentence.\\n<END_OF_USER_PROMPT>\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# you can use get_prompt or print_prompt\n",
        "\n",
        "prompt_kwargs = {\"input_str\": \"What is LLM? Explain in one sentence.\"}\n",
        "generator.get_prompt(**prompt_kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we only add the `input_str` so the prompt integrates the user query only, with the default system promp `You are a helpful assistant`.\n",
        "\n",
        "Next, let's call the generator:"
      ],
      "metadata": {
        "id": "ZzICss0YCNJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = generator(\n",
        "    prompt_kwargs=prompt_kwargs,\n",
        ")\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tg8Wf8mIPWu0",
        "outputId": "63a12c22-4e48-4b59-ceaf-c1d059b51b7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GeneratorOutput(id=None, data='LLM stands for Large Language Model, which refers to a type of artificial intelligence model designed to process and generate human-like language, often used in applications such as language translation, text summarization, and chatbots.', error=None, usage=CompletionUsage(completion_tokens=44, prompt_tokens=54, total_tokens=98), raw_response='LLM stands for Large Language Model, which refers to a type of artificial intelligence model designed to process and generate human-like language, often used in applications such as language translation, text summarization, and chatbots.', metadata=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generator with id\n",
        "\n",
        "Optionally, you can pass an id to keep track of your LLM call."
      ],
      "metadata": {
        "id": "_eFM7FpIujTl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = generator(\n",
        "    prompt_kwargs=prompt_kwargs, id =\"1\",\n",
        ")\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyLtHSMQs9MR",
        "outputId": "1a98e463-1810-40a5-d663-fcc56e65d6e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GeneratorOutput(id='1', data='LLM (Large Language Model) refers to a type of artificial intelligence designed to process and generate human-like language, using complex algorithms to analyze and understand vast amounts of text data.', error=None, usage=CompletionUsage(completion_tokens=37, prompt_tokens=54, total_tokens=91), raw_response='LLM (Large Language Model) refers to a type of artificial intelligence designed to process and generate human-like language, using complex algorithms to analyze and understand vast amounts of text data.', metadata=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"data: {output.data}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7geH58VQDy8",
        "outputId": "c0e7e7c7-d4b7-463c-9e7c-871c77a91114"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data: LLM (Large Language Model) refers to a type of artificial intelligence designed to process and generate human-like language, using complex algorithms to analyze and understand vast amounts of text data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generator output structure ``GeneratorOutput``"
      ],
      "metadata": {
        "id": "kInDhm6ZusqJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output will be the GeneratorOutput object. It has:\n",
        "- id (str)\n",
        "- data (object)\n",
        "- error (str)\n",
        "- raw_response (str)\n",
        "- metadata (dict)\n",
        "- usage\n",
        "\n",
        "This attributes make the usage of LLM output easier.\n",
        "\n",
        "Whether to do further processing or terminate the pipeline whenever an error occurs is up to the user from here on."
      ],
      "metadata": {
        "id": "ah9RSp5IPSIC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generator with Customized Template\n",
        "\n",
        "Let's customize the prompt here. We will use two variables `task_desc_str` and `input_str` in the prompt."
      ],
      "metadata": {
        "id": "k4YZVvx1RTKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = r\"\"\"<START_OF_SYSTEM_PROMPT>{{task_desc_str}}<END_OF_SYSTEM_PROMPT>\n",
        "<START_OF_USER_PROMPT>{{input_str}}<END_OF_USER_PROMPT>\"\"\"\n",
        "\n",
        "generator = adal.Generator(\n",
        "    model_client=GroqAPIClient(),\n",
        "    model_kwargs={\"model\": \"llama3-8b-8192\"},\n",
        "    template=template,\n",
        "    prompt_kwargs={\"task_desc_str\": \"You are a professional AI engineer.\"},\n",
        ")\n",
        "\n",
        "prompt_kwargs = {\"input_str\": \"Explain what is LLM to a non professional.\"}\n",
        "\n",
        "generator.get_prompt(\n",
        "    **prompt_kwargs,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "Addvu4MSPxv6",
        "outputId": "6f5a226f-01b9-47ff-8e45-72841f2decf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cache_path: /root/.adalflow/cache_GroqAPIClient_llama3-8b-8192.db\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<START_OF_SYSTEM_PROMPT>You are a professional AI engineer.<END_OF_SYSTEM_PROMPT>\\n<START_OF_USER_PROMPT>Explain what is LLM to a non professional.<END_OF_USER_PROMPT>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you will see the final prompt. To customize your own prompt, please stick to [Jinja2](https://jinja.palletsprojects.com/en/3.1.x/) syntax."
      ],
      "metadata": {
        "id": "fYOCZZHhSImG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = generator(\n",
        "    prompt_kwargs=prompt_kwargs,\n",
        ")\n",
        "print(output.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGGA9Qg4SG8B",
        "outputId": "a222181e-4689-44ad-a127-c4e9e9c64c9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'd be happy to explain Large Language Model (LLM) in simple terms!\n",
            "\n",
            "Imagine you have a super intelligent conversational partner who can understand and respond to any question you throw at it. This person is like a human expert, but instead of being a flesh-and-blood entity, it's a computer program designed to mimic human-like conversations.\n",
            "\n",
            "That's basically what an LLM is! It's a type of artificial intelligence (AI) that's trained on a massive amount of text data (think books, articles, social media posts, etc.). This training enables the LLM to learn patterns, relationships, and even underlying meanings in language.\n",
            "\n",
            "When you ask an LLM a question or provide some input, it uses this learned knowledge to generate a response that's often surprisingly accurate and natural-sounding. It's like having a conversational AI sidekick that can help with tasks, answer questions, or even engage in witty banter!\n",
            "\n",
            "Some common applications of LLMs include:\n",
            "\n",
            "1. Virtual assistants: Imagine asking your favorite chatbot for directions or recommendations, and it responds with an intelligent and helpful answer.\n",
            "2. Language translation: LLMs can be used to translate texts, websites, or even conversations across different languages.\n",
            "3. Content generation: They can be used to create articles, stories, or social media posts that sound like they were written by humans.\n",
            "4. Research assistance: LLMs can help researchers or students find relevant information, summarize texts, or even generate new ideas.\n",
            "\n",
            "These super-smart language models are constantly evolving and improving, opening up new possibilities for how we interact with technology and how AI can benefit our daily lives.\n",
            "\n",
            "Would you like me to elaborate on anything specific about LLMs?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cache Generator\n",
        "\n",
        "In cases, especially when training with auto-optimizers, we want to cache calls to save both time and cost. You can turn on the ``use_cache`` in the generator.\n",
        "\n",
        "In default, AdalFlow will create a path at ~/.adalflow. Depends on your Os, you might have a different path."
      ],
      "metadata": {
        "id": "-UZ5MmrmvNFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check the cache path\n",
        "\n",
        "from adalflow.utils.global_config import get_adalflow_default_root_path\n",
        "\n",
        "get_adalflow_default_root_path()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "eNSfGJrIwIhw",
        "outputId": "89b2fe60-c9c7-4212-db5a-e8d863cddec2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/root/.adalflow'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generator = adal.Generator(\n",
        "    model_client=GroqAPIClient(),\n",
        "    model_kwargs={\"model\": \"llama3-8b-8192\"},\n",
        "    template=template,\n",
        "    prompt_kwargs={\"task_desc_str\": \"You are a professional AI engineer.\"},\n",
        "    use_cache=True,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpg4dayIviWt",
        "outputId": "b62250c0-204e-420c-9528-d7b8600d9d0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cache_path: /root/.adalflow/cache_GroqAPIClient_llama3-8b-8192.db\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = generator(\n",
        "    prompt_kwargs=prompt_kwargs,\n",
        ")\n",
        "print(output.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5GWrj4CvvCI",
        "outputId": "854e4bd3-c7be-412f-847c-8df15dc95a72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Large Language Model (LLM) refers to a type of artificial intelligence model that is specifically designed to analyze, understand, and generate human-like text, often using vast amounts of training data to improve its language processing capabilities.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# when we run this query again, the response is still same as it is cached.\n",
        "output = generator(\n",
        "    prompt_kwargs=prompt_kwargs,\n",
        ")\n",
        "print(output.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G82QPutevzM3",
        "outputId": "f0ab1b84-f8cc-4837-9965-45c232d8b45a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Large Language Model (LLM) refers to a type of artificial intelligence model that is specifically designed to analyze, understand, and generate human-like text, often using vast amounts of training data to improve its language processing capabilities.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### With Output Processor\n",
        "\n",
        "Different tasks require LLM to output differently. We want to provide developers with powerful choices out of box yet still allow you to customize.  \n",
        "\n",
        "There are three types of pasers:\n",
        "(1) basic string parsers, it only convert a string to a certain data type. There are float, int, and json/yaml parsers that can convert yaml or json string to yaml/json object. [String parser API reference](https://adalflow.sylph.ai/apis/core/core.string_parser.html#module-core.string_parser)\n",
        "\n",
        "(2) Advanced output parser that will work with ``DataClass``, offering both output_format_str that instructs the LLM to output json/yaml and the ability to parse it back to structured data, either dict or ``DataClass``. [Output parser API reference](https://adalflow.sylph.ai/apis/components/components.output_parsers.outputs.html)\n",
        "\n",
        "(3) The most hands-off ``DataClass`` parser, which can understand the ``input_fields`` and ``output_fields`` of a data class. [DataClass Parser API reference](https://adalflow.sylph.ai/apis/components/components.output_parsers.dataclass_parser.html)\n",
        "\n",
        "\n",
        "Also, output parsers are just a component. You can customize it easily, and you can even chain together multiple steps of processing using ``adal.Sequential``."
      ],
      "metadata": {
        "id": "qSHZRJZHTEVR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Parser"
      ],
      "metadata": {
        "id": "ZhazEuzByC3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let LLM output json string, and convert it back to dict\n",
        "\n",
        "output_format_str = r\"\"\"Your output should be formatted as a standard JSON object with two keys:\n",
        "{\n",
        "    \"explanation\": \"A brief explanation of the concept in one sentence.\",\n",
        "    \"example\": \"An example of the concept in a sentence.\"\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "generator = adal.Generator(\n",
        "    model_client=GroqAPIClient(),\n",
        "    model_kwargs={\"model\": \"llama3-8b-8192\"},\n",
        "    prompt_kwargs={\"output_format_str\": output_format_str},\n",
        "    output_processors=adal.JsonParser(),\n",
        ")\n",
        "\n",
        "generator(prompt_kwargs=prompt_kwargs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YNGPNtqyHbc",
        "outputId": "81f306f5-53ab-42e7-f448-ed757dd751d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cache_path: /root/.adalflow/cache_GroqAPIClient_llama3-8b-8192.db\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GeneratorOutput(id=None, data={'explanation': 'LLM stands for Large Language Model, a type of AI model that is trained on vast amounts of text data to generate human-like responses and understand natural language.', 'example': 'The new large language model was able to answer complex questions and create coherent paragraphs with ease.'}, error=None, usage=CompletionUsage(completion_tokens=62, prompt_tokens=108, total_tokens=170), raw_response='{\\n\"explanation\": \"LLM stands for Large Language Model, a type of AI model that is trained on vast amounts of text data to generate human-like responses and understand natural language.\",\\n\"example\": \"The new large language model was able to answer complex questions and create coherent paragraphs with ease.\"\\n}', metadata=None)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can check out the `raw response`(in json string) and the parsed output at `data` which is a dict. Let's try yaml."
      ],
      "metadata": {
        "id": "8cxF7wlh0jE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_format_str = r\"\"\"Your output should be formatted as a standard JSON object with two keys:\n",
        "{\n",
        "    \"explanation\": \"A brief explanation of the concept in one sentence.\",\n",
        "    \"example\": \"An example of the concept in a sentence.\"\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "generator = adal.Generator(\n",
        "    model_client=GroqAPIClient(),\n",
        "    model_kwargs={\"model\": \"llama3-8b-8192\"},\n",
        "    prompt_kwargs={\"output_format_str\": output_format_str},\n",
        "    output_processors=adal.YamlParser(),\n",
        ")\n",
        "\n",
        "generator(prompt_kwargs=prompt_kwargs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72OYLuSq0khs",
        "outputId": "cf179651-e315-49ad-d87e-16bae65cf523"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cache_path: /root/.adalflow/cache_GroqAPIClient_llama3-8b-8192.db\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GeneratorOutput(id=None, data={'explanation': 'LLM (Large Language Model) refers to a type of artificial intelligence model that is trained on large amounts of text data to generate human-like language and understand natural language processing.', 'example': \"LLMs are used in virtual assistants like Alexa and Google Assistant to understand and respond to users' voice commands.\"}, error=None, usage=CompletionUsage(completion_tokens=71, prompt_tokens=108, total_tokens=179), raw_response='{\\n  \"explanation\": \"LLM (Large Language Model) refers to a type of artificial intelligence model that is trained on large amounts of text data to generate human-like language and understand natural language processing.\",\\n  \"example\": \"LLMs are used in virtual assistants like Alexa and Google Assistant to understand and respond to users\\' voice commands.\"\\n}', metadata=None)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output is a yaml string, with the data in dict."
      ],
      "metadata": {
        "id": "9CRYhF2y05_U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### OutputParser\n",
        "\n",
        "In the last example, we have to write our own output format str. We can streamline this with ``DataClass`` along with advanced output parsers: ``adal.JsonOutputParser`` and ``adal.YamlOutputParser``."
      ],
      "metadata": {
        "id": "FZGrrc121EOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass, field\n",
        "\n",
        "@dataclass\n",
        "class QAOutput(adal.DataClass):\n",
        "    explanation: str = field(\n",
        "        metadata={\"desc\": \"A brief explanation of the concept in one sentence.\"}\n",
        "    )\n",
        "    example: str = field(metadata={\"desc\": \"An example of the concept in a sentence.\"})\n",
        "    question: str = field(metadata={\"desc\": \"The question asked.\"}, default=None) # optional with default\n",
        "\n",
        "    __input_fields__ = [\"question\"]\n",
        "    __output_fields__ = [\"explanation\", \"example\"]"
      ],
      "metadata": {
        "id": "MVBOzTDA1NFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to first config the parser, this time, we want the data to be QAOutput directly.\n",
        "\n",
        "It is not necessarily with ``__input__fields__`` and ``__output_fields__``.\n",
        "\n",
        "Note:\n",
        "\n",
        "*Output parser does not directly understand input or output fields, if you want to get data format str for explanation and example, you can use either `include_fields` or `exclude_fields`.*"
      ],
      "metadata": {
        "id": "mIDvqjLz3HP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# see json format instruction asking\n",
        "parser = adal.JsonOutputParser(data_class=QAOutput, return_data_class=True, include_fields=[\"explanation\", \"example\"])\n",
        "\n",
        "parser.format_instructions()\n",
        "\n",
        "# In default, it uses signature, which is the simplified schema that simulates the json output format and field description\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "4c-NPn-52ofl",
        "outputId": "630bc399-6b4c-4fc7-8e55-3fa33e467836"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Your output should be formatted as a standard JSON instance with the following schema:\\n```\\n{\\n    \"explanation\": \"A brief explanation of the concept in one sentence. (str) (required)\",\\n    \"example\": \"An example of the concept in a sentence. (str) (required)\"\\n}\\n```\\n-Make sure to always enclose the JSON output in triple backticks (```). Please do not add anything other than valid JSON output!\\n-Use double quotes for the keys and string values.\\n-DO NOT mistaken the \"properties\" and \"type\" in the schema as the actual fields in the JSON output.\\n-Follow the JSON formatting conventions.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from adalflow.core.base_data_class import DataClassFormatType\n",
        "\n",
        "# the most comprehensive schema\n",
        "parser.format_instructions(format_type=DataClassFormatType.SCHEMA)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "zBRqIvbx4oYe",
        "outputId": "b8e6271b-523d-45a8-a285-3cf3d7396ee7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Your output should be formatted as a standard JSON instance with the following schema:\\n```\\n{\\n    \"type\": \"QAOutput\",\\n    \"properties\": {\\n        \"explanation\": {\\n            \"type\": \"str\",\\n            \"desc\": \"A brief explanation of the concept in one sentence.\"\\n        },\\n        \"example\": {\\n            \"type\": \"str\",\\n            \"desc\": \"An example of the concept in a sentence.\"\\n        }\\n    },\\n    \"required\": [\\n        \"explanation\",\\n        \"example\"\\n    ]\\n}\\n```\\n-Make sure to always enclose the JSON output in triple backticks (```). Please do not add anything other than valid JSON output!\\n-Use double quotes for the keys and string values.\\n-DO NOT mistaken the \"properties\" and \"type\" in the schema as the actual fields in the JSON output.\\n-Follow the JSON formatting conventions.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# try exclude fields with question\n",
        "parser = adal.JsonOutputParser(data_class=QAOutput, return_data_class=True, exclude_fields=[\"question\"])\n",
        "\n",
        "parser.format_instructions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "AJZ4idip5Ta6",
        "outputId": "0b1783e9-e232-470f-96d5-126a2124af46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Your output should be formatted as a standard JSON instance with the following schema:\\n```\\n{\\n    \"explanation\": \"A brief explanation of the concept in one sentence. (str) (required)\",\\n    \"example\": \"An example of the concept in a sentence. (str) (required)\"\\n}\\n```\\n-Make sure to always enclose the JSON output in triple backticks (```). Please do not add anything other than valid JSON output!\\n-Use double quotes for the keys and string values.\\n-DO NOT mistaken the \"properties\" and \"type\" in the schema as the actual fields in the JSON output.\\n-Follow the JSON formatting conventions.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: the order of the fields in the output matters for the llm, especially when you use chain of thought, you want the rational/thought field ahead of the final prediction/answer.\n",
        "\n",
        "If you are using OutputParser, make sure you follow exactly the order you want."
      ],
      "metadata": {
        "id": "ZIGs-Jow5zg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def call_generator(parser):\n",
        "  generator = adal.Generator(\n",
        "      model_client=GroqAPIClient(),\n",
        "      model_kwargs={\"model\": \"llama3-8b-8192\"},\n",
        "      prompt_kwargs={\"output_format_str\": parser.format_instructions()},\n",
        "      output_processors=parser\n",
        "  )\n",
        "\n",
        "  output = generator(prompt_kwargs=prompt_kwargs)\n",
        "  print(output)\n",
        "\n",
        "call_generator(parser)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeDJ07gN34cv",
        "outputId": "64a33bd3-eb63-45e7-bfef-1a41dc8c306c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cache_path: /root/.adalflow/cache_GroqAPIClient_llama3-8b-8192.db\n",
            "GeneratorOutput(id=None, data=QAOutput(explanation='LLM stands for Large Language Model, a type of artificial intelligence algorithm that enables computers to process and understand natural language, learn from vast amounts of text data, and generate human-like responses.', example='LLMs are used in applications such as virtual assistants, chatbots, and language translation systems to provide accurate and context-specific responses.', question=None), error=None, usage=CompletionUsage(completion_tokens=80, prompt_tokens=195, total_tokens=275), raw_response='```\\n{\\n    \"explanation\": \"LLM stands for Large Language Model, a type of artificial intelligence algorithm that enables computers to process and understand natural language, learn from vast amounts of text data, and generate human-like responses.\",\\n    \"example\": \"LLMs are used in applications such as virtual assistants, chatbots, and language translation systems to provide accurate and context-specific responses.\"\\n}', metadata=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### DataClassParser\n",
        "\n",
        "Dataclass parser will understand ``__input__fields__`` and ``__output_fields__``. It's less customization and more hands-off. It uses either `yaml signature` or `json signature`."
      ],
      "metadata": {
        "id": "iqD0jVy96k8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parser = adal.DataClassParser(data_class=QAOutput, return_data_class=True, format_type=\"yaml\")\n",
        "\n",
        "print(parser.get_input_format_str())\n",
        "\n",
        "print(parser.get_output_format_str())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpV47Q5d6wcZ",
        "outputId": "e4429852-1678-4374-e533-f5631557cb89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "question: The question asked. (str) (optional)\n",
            "output_format_str: Your output should be formatted as a standard YAML instance with the following schema:\n",
            "```\n",
            "explanation: A brief explanation of the concept in one sentence. (str) (required)\n",
            "example: An example of the concept in a sentence. (str) (required)\n",
            "```\n",
            "-Make sure to always enclose the YAML output in triple backticks (```). Please do not add anything other than valid YAML output!\n",
            "-Follow the YAML formatting conventions with an indent of 2 spaces.\n",
            "-DO NOT mistaken the \"properties\" and \"type\" in the schema as the actual fields in the YAML output.\n",
            "-Quote the string values properly.\n",
            "Your output should be formatted as a standard YAML instance with the following schema:\n",
            "```\n",
            "explanation: A brief explanation of the concept in one sentence. (str) (required)\n",
            "example: An example of the concept in a sentence. (str) (required)\n",
            "```\n",
            "-Make sure to always enclose the YAML output in triple backticks (```). Please do not add anything other than valid YAML output!\n",
            "-Follow the YAML formatting conventions with an indent of 2 spaces.\n",
            "-DO NOT mistaken the \"properties\" and \"type\" in the schema as the actual fields in the YAML output.\n",
            "-Quote the string values properly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generator = adal.Generator(\n",
        "      model_client=GroqAPIClient(),\n",
        "      model_kwargs={\"model\": \"llama3-8b-8192\"},\n",
        "      prompt_kwargs={\"output_format_str\": parser.get_output_format_str()},\n",
        "      output_processors=parser\n",
        "  )\n",
        "\n",
        "output = generator(prompt_kwargs=prompt_kwargs)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwnRonnl7n2O",
        "outputId": "d3a22055-eabc-498a-d0e0-e1afc712c354"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output_format_str: Your output should be formatted as a standard YAML instance with the following schema:\n",
            "```\n",
            "explanation: A brief explanation of the concept in one sentence. (str) (required)\n",
            "example: An example of the concept in a sentence. (str) (required)\n",
            "```\n",
            "-Make sure to always enclose the YAML output in triple backticks (```). Please do not add anything other than valid YAML output!\n",
            "-Follow the YAML formatting conventions with an indent of 2 spaces.\n",
            "-DO NOT mistaken the \"properties\" and \"type\" in the schema as the actual fields in the YAML output.\n",
            "-Quote the string values properly.\n",
            "cache_path: /root/.adalflow/cache_GroqAPIClient_llama3-8b-8192.db\n",
            "GeneratorOutput(id=None, data=QAOutput(explanation='LLM is a type of artificial intelligence model that uses a large-scale neural network to generate human-like language.', example='LLM models can be used to create chatbots that respond to user queries in a conversational manner.', question=None), error=None, usage=CompletionUsage(completion_tokens=53, prompt_tokens=190, total_tokens=243), raw_response='```\\nexplanation: LLM is a type of artificial intelligence model that uses a large-scale neural network to generate human-like language.\\nexample: \"LLM models can be used to create chatbots that respond to user queries in a conversational manner.\"\\n```', metadata=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Customize parser\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xzHifGzG90eQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = r\"\"\"<SYS>\n",
        "{{system_prompt}}\n",
        "</SYS>\n",
        "<USER>\n",
        "{{input_str}}\n",
        "</USER>\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "@adal.fun_to_component\n",
        "def extract_answer_string(text: str):\n",
        "    \"\"\"Find Answer : $value from text\"\"\"\n",
        "    pattern = re.compile(r\".*?Answer\\s*:\\s*\\$?(\\d+)\")\n",
        "    match = pattern.search(text)\n",
        "\n",
        "    if match:\n",
        "        print(\"match\")\n",
        "        return match.group(0)\n",
        "    else:  # process the failure\n",
        "        print(\"no match\")\n",
        "        return text\n",
        "\n",
        "@adal.fun_to_component\n",
        "def extract_answer(text: str):\n",
        "    pattern = re.compile(r\"Answer\\s*:\\s*(.*)\")\n",
        "    match = pattern.search(text)\n",
        "\n",
        "    if match:\n",
        "        print(\"match\")\n",
        "        return match.group(1)\n",
        "    else:  # process the failure\n",
        "        print(\"no match\")\n",
        "        return text"
      ],
      "metadata": {
        "id": "M6i2JP7F98Q_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output1 = extract_answer_string(\"Answer: $1\")\n",
        "print(output1)\n",
        "output2 = extract_answer(output1)\n",
        "print(output2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3dWFZy_-AMU",
        "outputId": "8fd6e514-f4a0-40ac-b56a-a60d25be3c80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "match\n",
            "Answer: $1\n",
            "match\n",
            "$1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"Respond to a question in two lines (Do not use any start/end Tags):\n",
        "Rationale: Your step by step reasoning.\n",
        "Answer: ${answer}\"\"\"\n",
        "\n",
        "generator = adal.Generator(\n",
        "    model_client=GroqAPIClient(),\n",
        "    model_kwargs={\"model\": \"llama3-8b-8192\"},\n",
        "    template=template,\n",
        "    prompt_kwargs={\"system_prompt\": system_prompt},\n",
        "    output_processors=adal.Sequential(extract_answer_string, extract_answer)\n",
        ")\n",
        "\n",
        "prompt_kwargs = {\"input_str\": \"What is LLM? Explain in one sentence.\"}\n",
        "\n",
        "generator(prompt_kwargs=prompt_kwargs)\n",
        "\n",
        "# you can also just pass extract_answer_string, its really just taking a component."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XKtwTRD-Dn0",
        "outputId": "4706a783-496e-499b-85cd-b4d117885d2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cache_path: /root/.adalflow/cache_GroqAPIClient_llama3-8b-8192.db\n",
            "no match\n",
            "match\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GeneratorOutput(id=None, data='LLM is a type of AI model that generates human-like language responses by learning patterns and structures from a large corpus of text data.', error=None, usage=CompletionUsage(completion_tokens=65, prompt_tokens=62, total_tokens=127), raw_response='Rationale: LLM stands for Large Language Model, which is a type of AI model that is trained on a large corpus of text data to generate human-like language responses.\\nAnswer: LLM is a type of AI model that generates human-like language responses by learning patterns and structures from a large corpus of text data.', metadata=None)"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Client Selection\n",
        "\n",
        "AdalFlow provides an easy way to switch [`model_client`](https://adalflow.sylph.ai/tutorials/model_client.html)  in the Generator. We can even use [`ModelClientType`](https://adalflow.sylph.ai/apis/core/core.types.html#core.types.ModelClientType) to switch the model client without handling multiple imports. Let's see how both methods work:"
      ],
      "metadata": {
        "id": "s6rjfDPZ4_Fa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using openai model_client\n",
        "\n",
        "# install openai package\n",
        "!pip install -U openai\n",
        "\n",
        "clear_output()\n",
        "\n",
        "openai_api_key = getpass(\"Please enter your OpenAI API key: \")\n",
        "\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
        "\n",
        "\n",
        "\n",
        "# import the client\n",
        "from adalflow.components.model_client import OpenAIClient\n",
        "\n",
        "generator = adal.Generator(\n",
        "    model_client=OpenAIClient(),\n",
        "    model_kwargs={\"model\": \"gpt-3.5-turbo\"},\n",
        ")"
      ],
      "metadata": {
        "id": "yyvvnWUd4h9K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6acc8fd1-a74c-435f-f64d-1560994d8fc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter your OpenAI API key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "cache_path: /root/.adalflow/cache_OpenAIClient_gpt-3.5-turbo.db\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generator(prompt_kwargs=prompt_kwargs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoWOdOzF8oAA",
        "outputId": "212ddcd6-d93e-4351-b52d-42022c7abb8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GeneratorOutput(id=None, data='LLM stands for Large Language Model, which is a type of artificial intelligence model that is trained on large amounts of text data to generate human-like text.', error=None, usage=CompletionUsage(completion_tokens=31, prompt_tokens=51, total_tokens=82), raw_response='LLM stands for Large Language Model, which is a type of artificial intelligence model that is trained on large amounts of text data to generate human-like text.', metadata=None)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using ModelClientType\n",
        "\n",
        "# import the ModelClientType\n",
        "from adalflow.core.types import ModelClientType\n",
        "\n",
        "generator = adal.Generator(\n",
        "    model_client=ModelClientType.OPENAI(),  # or ModelClientType.GROQ()\n",
        "    model_kwargs={\"model\": \"gpt-3.5-turbo\"},\n",
        ")\n",
        "\n",
        "generator(prompt_kwargs=prompt_kwargs)"
      ],
      "metadata": {
        "id": "W721gqVf_owD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8156dd5e-58fe-4889-d9be-35a73f20d344"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cache_path: /root/.adalflow/cache_OpenAIClient_gpt-3.5-turbo.db\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GeneratorOutput(id=None, data='LLM stands for Large Language Model, a type of artificial intelligence model that is trained on vast amounts of text data to understand and generate human language.', error=None, usage=CompletionUsage(completion_tokens=30, prompt_tokens=51, total_tokens=81), raw_response='LLM stands for Large Language Model, a type of artificial intelligence model that is trained on vast amounts of text data to understand and generate human language.', metadata=None)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Easy Creation from Configs\n",
        "\n",
        "We can also create the generator purely from configs. There are 2 ways:\n",
        "- use `from_config` method from the `Generator` class"
      ],
      "metadata": {
        "id": "RsNZmJ7TABf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from adalflow.core import Generator\n",
        "\n",
        "config = {\n",
        "    \"model_client\": {\n",
        "        \"component_name\": \"GroqAPIClient\",\n",
        "        \"component_config\": {},\n",
        "    },\n",
        "    \"model_kwargs\": {\n",
        "        \"model\": \"llama3-8b-8192\",\n",
        "    },\n",
        "}\n",
        "\n",
        "generator: Generator = Generator.from_config(config)\n",
        "# print(generator)\n",
        "\n",
        "prompt_kwargs = {\"input_str\": \"What is LLM? Explain in one sentence.\"}\n",
        "# generator.print_prompt(**prompt_kwargs) # show the prompt\n",
        "output = generator(\n",
        "    prompt_kwargs=prompt_kwargs,\n",
        ")\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uV28_FI-ADSZ",
        "outputId": "1a5318b0-e610-45e0-a451-d06acdff92c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cache_path: /root/.adalflow/cache_GroqAPIClient_llama3-8b-8192.db\n",
            "GeneratorOutput(id=None, data='Large Language Model (LLM) is a type of artificial intelligence designed to process and generate human-like language, trained on vast amounts of text data to improve its understanding and production of language.', error=None, usage=CompletionUsage(completion_tokens=39, prompt_tokens=54, total_tokens=93), raw_response='Large Language Model (LLM) is a type of artificial intelligence designed to process and generate human-like language, trained on vast amounts of text data to improve its understanding and production of language.', metadata=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- create any component from configs. This is even more general. This method can be used to create any component from configs. We just need to follow the config structure: `component_name` and `component_config` for all arguments."
      ],
      "metadata": {
        "id": "2aYUizdvezpG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from adalflow.utils.config import new_component\n",
        "from adalflow.core import Generator\n",
        "\n",
        "config = {\n",
        "    \"generator\": {\n",
        "        \"component_name\": \"Generator\",\n",
        "        \"component_config\": {\n",
        "            \"model_client\": {\n",
        "                \"component_name\": \"GroqAPIClient\",\n",
        "                \"component_config\": {},\n",
        "            },\n",
        "            \"model_kwargs\": {\n",
        "                \"model\": \"llama3-8b-8192\",\n",
        "            },\n",
        "        },\n",
        "    }\n",
        "}\n",
        "\n",
        "generator: Generator = new_component(config[\"generator\"])\n",
        "# print(generator)\n",
        "\n",
        "prompt_kwargs = {\"input_str\": \"What is LLM? Explain in one sentence.\"}\n",
        "# generator.print_prompt(**prompt_kwargs)\n",
        "output = generator(\n",
        "    prompt_kwargs=prompt_kwargs,\n",
        ")\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CP5-6JDLfBnZ",
        "outputId": "d3608a64-dd7e-41be-d0ef-3c5957cb16a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cache_path: /root/.adalflow/cache_GroqAPIClient_llama3-8b-8192.db\n",
            "GeneratorOutput(id=None, data='LLM stands for Large Language Model, which refers to a type of artificial intelligence that is trained on vast amounts of text data to generate human-like language understanding and generation capabilities.', error=None, usage=CompletionUsage(completion_tokens=36, prompt_tokens=54, total_tokens=90), raw_response='LLM stands for Large Language Model, which refers to a type of artificial intelligence that is trained on vast amounts of text data to generate human-like language understanding and generation capabilities.', metadata=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It works exactly the same as the previous example. We imported Generator in this case to only show the type hinting."
      ],
      "metadata": {
        "id": "yhP1Rsu4fGEQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## QA Chatbot\n",
        "\n",
        "Now let's put it together and build our own QA chatbot application."
      ],
      "metadata": {
        "id": "K7puSJ9Eilgj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QA(adal.Component):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        parser = adal.DataClassParser(data_class=QAOutput, return_data_class=True)\n",
        "        self.generator = adal.Generator(\n",
        "            model_client=GroqAPIClient(),\n",
        "            model_kwargs={\"model\": \"llama3-8b-8192\"},\n",
        "            template=template,\n",
        "            prompt_kwargs={\"output_format_str\": parser.get_output_format_str()},\n",
        "            output_processors=parser,\n",
        "        )\n",
        "\n",
        "    def call(self, query: str):\n",
        "        return self.generator.call({\"input_str\": query})\n",
        "\n",
        "    async def acall(self, query: str):\n",
        "        return await self.generator.acall({\"input_str\": query})\n",
        "\n",
        "\n",
        "qa_bot = QA()\n",
        "answer = qa_bot(\"What is LLM?\")\n",
        "print(qa_bot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDzr6Vsji1xF",
        "outputId": "3eb6729f-3278-4592-8673-4331f8d1c4c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output_format_str: Your output should be formatted as a standard JSON instance with the following schema:\n",
            "```\n",
            "{\n",
            "    \"explanation\": \"A brief explanation of the concept in one sentence. (str) (required)\",\n",
            "    \"example\": \"An example of the concept in a sentence. (str) (required)\"\n",
            "}\n",
            "```\n",
            "-Make sure to always enclose the JSON output in triple backticks (```). Please do not add anything other than valid JSON output!\n",
            "-Use double quotes for the keys and string values.\n",
            "-DO NOT mistaken the \"properties\" and \"type\" in the schema as the actual fields in the JSON output.\n",
            "-Follow the JSON formatting conventions.\n",
            "cache_path: /root/.adalflow/cache_GroqAPIClient_llama3-8b-8192.db\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:adalflow.components.output_parsers.dataclass_parser:Error at parsing output: Error: No JSON object or array found in the text: You're asking about LLM!\n",
            "\n",
            "LLM stands for Large Language Model. It's a type of artificial intelligence (AI) designed to process and understand human language. These models are trained on vast amounts of text data and can generate human-like language responses, complete sentences, or even entire paragraphs.\n",
            "\n",
            "LLMs are typically built using specialized algorithms and deep learning techniques, which allow them to learn and improve their language understanding and generation capabilities over time. They can be used in a wide range of applications, such as:\n",
            "\n",
            "1. Language translation: LLMs can translate text from one language to another with remarkable accuracy.\n",
            "2. Text summarization: They can condense long pieces of text into shorter, more digestible summaries.\n",
            "3. Chatbots and customer service: LLMs can be used to power conversational interfaces, helping customers with inquiries and support requests.\n",
            "4. Content generation: They can generate content, such as news articles, product descriptions, or social media posts, often in a few seconds.\n",
            "5. Language analysis: LLMs can analyze language patterns, sentiment, and intent, making them useful for applications like sentiment analysis, text classification, and topic modeling.\n",
            "\n",
            "Some notable examples of LLMs include:\n",
            "\n",
            "1. Transformer-based models like BERT, RoBERTa, and XLNet, which have achieved state-of-the-art results in various natural language processing tasks.\n",
            "2. Language models like OpenAI's GPT-3, which can generate human-like text and even create entire books.\n",
            "\n",
            "If you have any specific questions about LLMs or want to know more about a particular application, feel free to ask!\n",
            "ERROR:adalflow.core.generator:Error processing the output processors: Error: Error: No JSON object or array found in the text: You're asking about LLM!\n",
            "\n",
            "LLM stands for Large Language Model. It's a type of artificial intelligence (AI) designed to process and understand human language. These models are trained on vast amounts of text data and can generate human-like language responses, complete sentences, or even entire paragraphs.\n",
            "\n",
            "LLMs are typically built using specialized algorithms and deep learning techniques, which allow them to learn and improve their language understanding and generation capabilities over time. They can be used in a wide range of applications, such as:\n",
            "\n",
            "1. Language translation: LLMs can translate text from one language to another with remarkable accuracy.\n",
            "2. Text summarization: They can condense long pieces of text into shorter, more digestible summaries.\n",
            "3. Chatbots and customer service: LLMs can be used to power conversational interfaces, helping customers with inquiries and support requests.\n",
            "4. Content generation: They can generate content, such as news articles, product descriptions, or social media posts, often in a few seconds.\n",
            "5. Language analysis: LLMs can analyze language patterns, sentiment, and intent, making them useful for applications like sentiment analysis, text classification, and topic modeling.\n",
            "\n",
            "Some notable examples of LLMs include:\n",
            "\n",
            "1. Transformer-based models like BERT, RoBERTa, and XLNet, which have achieved state-of-the-art results in various natural language processing tasks.\n",
            "2. Language models like OpenAI's GPT-3, which can generate human-like text and even create entire books.\n",
            "\n",
            "If you have any specific questions about LLMs or want to know more about a particular application, feel free to ask!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "call back on failure: GeneratorOutput(id=None, data=None, error=\"Error: Error: No JSON object or array found in the text: You're asking about LLM!\\n\\nLLM stands for Large Language Model. It's a type of artificial intelligence (AI) designed to process and understand human language. These models are trained on vast amounts of text data and can generate human-like language responses, complete sentences, or even entire paragraphs.\\n\\nLLMs are typically built using specialized algorithms and deep learning techniques, which allow them to learn and improve their language understanding and generation capabilities over time. They can be used in a wide range of applications, such as:\\n\\n1. Language translation: LLMs can translate text from one language to another with remarkable accuracy.\\n2. Text summarization: They can condense long pieces of text into shorter, more digestible summaries.\\n3. Chatbots and customer service: LLMs can be used to power conversational interfaces, helping customers with inquiries and support requests.\\n4. Content generation: They can generate content, such as news articles, product descriptions, or social media posts, often in a few seconds.\\n5. Language analysis: LLMs can analyze language patterns, sentiment, and intent, making them useful for applications like sentiment analysis, text classification, and topic modeling.\\n\\nSome notable examples of LLMs include:\\n\\n1. Transformer-based models like BERT, RoBERTa, and XLNet, which have achieved state-of-the-art results in various natural language processing tasks.\\n2. Language models like OpenAI's GPT-3, which can generate human-like text and even create entire books.\\n\\nIf you have any specific questions about LLMs or want to know more about a particular application, feel free to ask!\", usage=CompletionUsage(completion_tokens=327, prompt_tokens=43, total_tokens=370), raw_response=\"You're asking about LLM!\\n\\nLLM stands for Large Language Model. It's a type of artificial intelligence (AI) designed to process and understand human language. These models are trained on vast amounts of text data and can generate human-like language responses, complete sentences, or even entire paragraphs.\\n\\nLLMs are typically built using specialized algorithms and deep learning techniques, which allow them to learn and improve their language understanding and generation capabilities over time. They can be used in a wide range of applications, such as:\\n\\n1. Language translation: LLMs can translate text from one language to another with remarkable accuracy.\\n2. Text summarization: They can condense long pieces of text into shorter, more digestible summaries.\\n3. Chatbots and customer service: LLMs can be used to power conversational interfaces, helping customers with inquiries and support requests.\\n4. Content generation: They can generate content, such as news articles, product descriptions, or social media posts, often in a few seconds.\\n5. Language analysis: LLMs can analyze language patterns, sentiment, and intent, making them useful for applications like sentiment analysis, text classification, and topic modeling.\\n\\nSome notable examples of LLMs include:\\n\\n1. Transformer-based models like BERT, RoBERTa, and XLNet, which have achieved state-of-the-art results in various natural language processing tasks.\\n2. Language models like OpenAI's GPT-3, which can generate human-like text and even create entire books.\\n\\nIf you have any specific questions about LLMs or want to know more about a particular application, feel free to ask!\", metadata=None)\n",
            "QA(\n",
            "  (generator): Generator(\n",
            "    model_kwargs={'model': 'llama3-8b-8192'}, \n",
            "    (prompt): Prompt(\n",
            "      template: <START_OF_SYSTEM_PROMPT>{{task_desc_str}}<END_OF_SYSTEM_PROMPT>\n",
            "      <START_OF_USER_PROMPT>{{input_str}}<END_OF_USER_PROMPT>, prompt_kwargs: {'output_format_str': 'Your output should be formatted as a standard JSON instance with the following schema:\\n```\\n{\\n    \"explanation\": \"A brief explanation of the concept in one sentence. (str) (required)\",\\n    \"example\": \"An example of the concept in a sentence. (str) (required)\"\\n}\\n```\\n-Make sure to always enclose the JSON output in triple backticks (```). Please do not add anything other than valid JSON output!\\n-Use double quotes for the keys and string values.\\n-DO NOT mistaken the \"properties\" and \"type\" in the schema as the actual fields in the JSON output.\\n-Follow the JSON formatting conventions.'}, prompt_variables: ['input_str', 'task_desc_str']\n",
            "    )\n",
            "    (model_client): GroqAPIClient()\n",
            "    (output_processors): DataClassParser(\n",
            "      data_class=QAOutput, format_type=json,            return_data_class=True, input_fields=['question'],            output_fields=['explanation', 'example']\n",
            "      (_output_processor): JsonParser()\n",
            "      (output_format_prompt): Prompt(\n",
            "        template: Your output should be formatted as a standard JSON instance with the following schema:\n",
            "        ```\n",
            "        {{schema}}\n",
            "        ```\n",
            "        -Make sure to always enclose the JSON output in triple backticks (```). Please do not add anything other than valid JSON output!\n",
            "        -Use double quotes for the keys and string values.\n",
            "        -DO NOT mistaken the \"properties\" and \"type\" in the schema as the actual fields in the JSON output.\n",
            "        -Follow the JSON formatting conventions., prompt_variables: ['schema']\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXmI7Lkkjq9q",
        "outputId": "bda48590-8d1a-4e12-eb1e-316732618d81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GeneratorOutput(data=QAOutput(explanation='LLM stands for Large Language Model, which refers to a type of artificial intelligence that is trained on a massive scale to generate and understand human-like language.', example='For instance, an LLM can be used to generate automated customer support responses or even help write news articles.'), error=None, usage=None, raw_response='```\\n{\\n    \"explanation\": \"LLM stands for Large Language Model, which refers to a type of artificial intelligence that is trained on a massive scale to generate and understand human-like language.\",\\n    \"example\": \"For instance, an LLM can be used to generate automated customer support responses or even help write news articles.\"\\n}\\n```', metadata=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Examples Across the Library\n",
        "\n",
        "Besides these examples, LLM is like water, even in our library, we have components that have adpated Generator to various other functionalities.\n",
        "\n",
        "Checklist:\n",
        "- [LLMRetriever](https://lightrag.sylph.ai/apis/components/components.retriever.llm_retriever.html#components.retriever.llm_retriever.LLMRetriever)\n",
        "- [DefaultLLMJudge](https://lightrag.sylph.ai/apis/eval/eval.llm_as_judge.html#eval.llm_as_judge.DefaultLLMJudge)"
      ],
      "metadata": {
        "id": "A4-teQITfcjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tracing\n",
        "\n",
        "We provide two tracing methods to help you develop and improve the Generator:\n",
        "- Trace the history change (states) on prompt during your development process.\n",
        "- Trace all failed LLM predictions for further improvement.\n",
        "\n",
        "Please refer to the [tracing](https://lightrag.sylph.ai/tutorials/logging_tracing.html) to learn about these two tracing methods."
      ],
      "metadata": {
        "id": "3FDVsvKCf9kO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Issues and feedback\n",
        "\n",
        "If you encounter any issues, please report them here: [GitHub Issues](https://github.com/SylphAI-Inc/LightRAG/issues).\n",
        "\n",
        "For feedback, you can use either the [GitHub discussions](https://github.com/SylphAI-Inc/LightRAG/discussions) or [Discord](https://discord.gg/ezzszrRZvT)."
      ],
      "metadata": {
        "id": "QsU48ucvfgmZ"
      }
    }
  ]
}